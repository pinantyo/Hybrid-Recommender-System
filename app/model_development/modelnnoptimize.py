# -*- coding: utf-8 -*-
"""modelNNOptimize.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jag4NtYJ0bsleXGNFIy2Xjnce_oSPQgA

# **Skripsi**
---
Nama : Steven Christ Pinantyo Arwidarasto

Topik: Rekomendasi Tempat Wisata Menggunakan Metode Hybrid
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!pip install -q --upgrade tensorflow-datasets pydot pydotplus graphviz

"""# Data Benchmark"""

import pandas as pd
import numpy as np

# Dataset
import tensorflow_datasets as tfds

# Visualisasi
import seaborn as sns
import matplotlib.pyplot as plt

# Train test split
from sklearn.model_selection import train_test_split

import re
import os

# Pemodelan
import tensorflow as tf

from IPython.display import SVG

# Clear console
from IPython.display import clear_output
from keras.utils.vis_utils import model_to_dot

import requests

"""# Unzip & Read File

## Kaggle Dataset Benchmark

### Amazon Fashion
"""

!pip install -q kaggle

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/

import os

# Konfigurasi Direktori Kaggle
os.environ['KAGGLE_CONFIG_DIR'] = "/content/"

# !kaggle datasets download -d skillsmuggler/amazon-ratings
# !kaggle datasets download -d ritvik1909/indian-places-to-visit-reviews-data

# !unzip '/content/amazon-ratings.zip'

"""### Movielens"""

movielens_img = pd.read_csv('/content/drive/MyDrive/Colab/Sistem Rekomendasi/Datasets/Movielens/movie_poster.csv')
movielens_movie = pd.read_csv('/content/drive/MyDrive/Colab/Sistem Rekomendasi/Datasets/Movielens/movies.csv')
movielens_rating = pd.read_csv('/content/drive/MyDrive/Colab/Sistem Rekomendasi/Datasets/Movielens/ratings.csv')

movielens_rating.head()

movielens_movie.head()

movielens_img.head()

# Merging datasets
df_benchmark = movielens_rating.merge(
    movielens_movie, 
    left_on='movieId',
    right_on='movieId'
)

df_benchmark.head()

movielens_movie.shape

# # Pegunduhan data gambar
# for count, i in enumerate(movielens_img['URL.1'].values):
#   img_data = requests.get(i).content
#   with open(f'/content/drive/MyDrive/Colab/Sistem Rekomendasi/Datasets/Movielens/images/{count}.jpg', 'wb') as handler:
#       handler.write(img_data)

img_path = '/content/drive/MyDrive/Colab/Sistem Rekomendasi/Datasets/Movielens/images'
img_path = [img_path+i for i in os.listdir(img_path)]
print(len(img_path))

# img_movielens = np.array([cv2.cvtColor(cv2.imread(i), cv2.COLOR_BGR2RGB) for i in img_path])

# img_movielens /= 255.0

"""## Data Transaction Tourism"""

ratings = pd.read_csv('/content/drive/MyDrive/Colab/Sistem Rekomendasi/Datasets/IndonesianTourismDataset/tourism_rating.csv')
tourism = pd.read_csv('/content/drive/MyDrive/Colab/Sistem Rekomendasi/Datasets/IndonesianTourismDataset/tourism_with_id.csv')
user = pd.read_csv('/content/drive/MyDrive/Colab/Sistem Rekomendasi/Datasets/IndonesianTourismDataset/user.csv')

# Merging tiga data
df = ratings.merge(
    tourism, 
    left_on='Place_Id',
    right_on='Place_Id'
)

df = df.merge(
    user,
    left_on='User_Id',
    right_on='User_Id'
)


# df = pd.read_csv('/content/drive/MyDrive/Colab/Sistem Rekomendasi/UserVisitTourism.csv')
df.sort_values(by=['Place_Id'], ascending=True, inplace=True)
df.head()

"""## Data Image Tourism"""

import cv2
import os

def resize_img(img, size):
  return cv2.resize(img, (size,size))

size = 128
path_data_imgs = '/content/drive/MyDrive/Colab/ImageRecommendation/images'
data_images = [os.path.join(path_data_imgs, f'{i}.jpg') for i in range(1, len(os.listdir(path_data_imgs))+1)]


tourism_images = np.array([cv2.cvtColor(resize_img(cv2.imread(i),size),cv2.COLOR_BGR2RGB) for i in data_images])

data_images

# Normalisasi gambar [0, 1]
tourism_images = tourism_images/255.0
tourism_images.shape

df['Image_Feature'] = df['Place_Id'].map(lambda x:tourism_images[x-1])

tf.expand_dims(
    df['Image_Feature'].values[0], 
    axis=0,
)

"""# Exploratory Data Analysis

> Berdasarkan data, ditemukan terdapat kolom rating dengan range 1-5 yang akan digunakan sebagai target prediksi rating yang diberikan oleh user kepada tempat wisata
"""

df.columns

df.Place_Ratings.unique()

"""> Terdapat jumlah transaksi sebesar 10000 dengan nilai null pada kolom fullname dan time_minutes. Untuk halnya time_minutes, akan dilakukan penghapusan kolom dikarenakan kolom tersebut hanya mendeskripsikan lama waktu penunjung dalam berwisata di tempat tersebut.

> Adapun kolom fullname yang tidak memberikan relasi behavioural pengunjung kepada tempat wisata
"""

# Columns type and total null
df.info()

# Check null columns
df.isnull().sum()

"""> Berdasarkan pengecekan duplikasi, terdapat banyak duplikasi pada kolom-kolom tersebut. Hal ini didasari dengan data yang digunakan merupakan data hasil penggabungan kolom user, tourism, dan kolom transaksi user-tourism."""

# Check duplicate
print(f'Duplicate {df.iloc[:,:-1].duplicated().sum()}')

# Check total unique
print(df.iloc[:,:-1].nunique())

"""> Selanjutnya, berdasarkan data, ditemukan bahwa tempat wisata terpopuler berdasarkan ratingsnya (tidak dengan jumlah pengunjung) jatuh kepada tempat wisata dengan id 416, yang diikuti dengan id 139, dan 52. Adapun rata-rata rating tertinggi yaitu 3.97."""

# Popularity Based
pd.pivot_table(df[['Place_Id','Place_Ratings']],index='Place_Id',aggfunc='mean').sort_values(by=['Place_Ratings'],ascending=False).head()

"""> Berdasarkan data korelasi antar kolom, setiap kolom memiliki relasi yang lemah sampai kepada relasi yang tidak signifikan. Hal ini menunjukan bahwa variasi pada data terbilang sangat tinggi sehingga tidak ditemukannya pola secara umum."""

plt.figure(figsize=[10,10])
sns.heatmap(df.iloc[:,:-1].corr(),cmap="RdYlGn", annot=True)
plt.show()

"""> Adapun tempat wisata yang ramai dikunjungi oleh banyak orang berdasarkan data tersebut, antara lain;
* ID 177 sebanyak 39x dikunjungi
* ID 298 sebanyak 39x dikunjungi
* ID 437 sebanyak 38x dikunjungi
"""

df.iloc[:,:-1].groupby('Place_Id')['User_Id'].count().sort_values(ascending=False)[:10]

"""> Selanjutnya, akan dilakukan pengecekan distribusi data untuk mengetahui adanya outlier dan dimana mayoritas data berada"""

from scipy import stats

# Pengecekan distribusi data
def perhitunganZScore(data, columns):
    for i in columns:
        z_score = stats.zscore(data[i])
        print(f'Kolom {i} memiliki nilai zscore {stats.kstest(z_score, "norm")}\nStatus: {["Terdistribusi normal" if stats.kstest(z_score, "norm")[1] > 0.05 else "Tidak terdistribusi normal"]}\n')

# visualisasi boxplot 
def boxplotVisualisasi(data, columns):
    plt.figure(figsize=(15,8))
    for i, colom in enumerate(columns):
        plt.subplot(2, int(len(columns)/2), i+1)
        sns.boxplot(x=data[colom])
        plt.tight_layout()

# visualisasi histplot
def histplotVisualisasi(data, columns):
    plt.figure(figsize=(15,8))
    for i, colom in enumerate(columns):
        plt.subplot(2, int(len(columns)/2), i+1)
        sns.histplot(data=data, x=colom,kde=True)
        plt.tight_layout()
        
# Penghapusan outlier
def penghapusanOutlier(data):
    Q1=data.quantile(0.25)
    Q3=data.quantile(0.75)
    IQR=Q3-Q1
    return data[~((data<(Q1-1.5*IQR)) | (data>(Q3+1.5*IQR)))].dropna() # penghapusan nilai null

"""> Penyebaran data ratings telah terbukti tidak terdistribusi normal, hal ini menunjukan bahwa mayoritas data memiliki kemungkinan terdapat pada ratings tinggi atau ratings yang rendah"""

perhitunganZScore(df, ['User_Id','Place_Id','Place_Ratings'])

# Pengambilan kolom int64
numeric_cols = []
for i in df.columns:
  if df[i].dtype == 'int64':
    numeric_cols.append(i)
numeric_cols

"""> Pengecekan adanya outlier
---
Berdasarkan visualisasi boxplot, tidak ditemukannya outlier pada data-data integer, hal ini membuat, tidak dibutuhkannya penghapusan atau normalisasi outlier untuk dilakukan
"""

boxplotVisualisasi(df, numeric_cols)

histplotVisualisasi(df, numeric_cols)

"""> Mengambil preferensi pengguna berdasarkan rata-rata pemberian """

mean_ratings = pd.DataFrame(df.groupby(['User_Id','Category'])['Place_Ratings'].mean())
mean_ratings.head()

"""# Data Preprocessing

> Berdasarkan EDA yang telah dilakukan, maka akan dilakukannya pembersihan nilai null dan penghapusan kolom yang tidak digunakan. Kolom tersebut antara lain;
* Long
* Lat
* Unnammed: 0
* Fullname
* Time_Minutes
"""

# Drop null columns and unecessary columns
df.dropna(axis=1, inplace=True)

try:
  df.drop(inplace=True, columns=['Unnamed: 0','Unnamed: 11','Unnamed: 12'])
except:
  print('Kolom tidak ditemukan')
else:
  print('Kolom berhasil dihapus')

"""> Untuk nilai ratings [1-5] akan dilakukan normalisasi ***minmax*** ke dalam range [0-1] untuk memudahkan model dalam melakukan pembelajaran data serta mengoptimasi waktu yang dibutuhkan pada saat melakukan training"""

def normalizationMinMax(data, targetColumn, minVal=0, maxVal=1):
  min = minVal
  max = maxVal
  ratingmin = df['Place_Ratings'].min()
  ratingmax = df['Place_Ratings'].max()
  df['Place_Ratings'] = ((df['Place_Ratings']-ratingmin)*(max-min)/(ratingmax-ratingmin))+min
  return df['Place_Ratings']

normalizationMinMax(df, 'Place_Ratings',0,1)

"""> Selanjutnya akan dilakukan pengambilan preferensi berdasarkan pemberian rata-rata ratings oleh tiap pengunjung.

> Adapun threshold yang digunakan yaitu ratings diatas atau sama dengan 3.0
"""

# mean_ratings['User_Id'] = np.array([i[0] for i in mean_ratings.index])
# mean_ratings['Category'] = np.array([i[1] for i in mean_ratings.index])
mean_ratings.reset_index(inplace=True)
mean_ratings.sort_values(by=['Place_Ratings'], ascending=False, inplace=True)

preferensi_dict = {}

for i in mean_ratings.User_Id.unique():
  user_rating = mean_ratings[mean_ratings['User_Id'] == i]
  mean = user_rating['Place_Ratings'].mean()
  list_preference = list(user_rating[user_rating['Place_Ratings'] > mean]['Category'].values)

  preferensi_dict[i] = list_preference

preferensi_dict

print(len(list(preferensi_dict.keys())))

"""> Berdasarkan preferensi diatas, telah ditemukan terdapat 298 user yang memiliki pola preferensi diatas. 

> Sisa dari kedua user dibawah threshold akan menggunakan nilai tertinggi pemberian rating pada kategori tersebut
"""

preferensi_dict_normalized = {}
for i in list(df.Category.unique()):
  if i not in list(preferensi_dict_normalized.keys()):
    preferensi_dict_normalized[i] = []
  
  for j in list(preferensi_dict.values()):
    if i in j:
      preferensi_dict_normalized[i].append(1)
    else:
      preferensi_dict_normalized[i].append(0)

preferensi_dict_normalized['User_Id'] = list(preferensi_dict.keys())

preferensi_user = pd.DataFrame(preferensi_dict_normalized, columns=list(df.Category.unique())+['User_Id'])
preferensi_user.columns = [f'P_{i}' for i in preferensi_user.columns]
preferensi_user

# Sortir bedarsarkan id user
df.sort_values(by=['User_Id'], ascending=True, inplace=True)

merged_preferensi = df.merge(
    preferensi_user, 
    how='left', 
    left_on='User_Id', 
    right_on='P_User_Id', 
    copy=True, indicator=False, validate=None)

merged_preferensi.drop(inplace=True, columns=['P_User_Id'])

merged_preferensi.isnull().sum()

"""> Adapun kolom-kolom yang memiliki tipe data kategorik seperti dibawah ini. Kolom-kolom tersebut akan dipilih dan dibuatkan kolom dummy yang merepresentasikan nilai kategoriknya. Adapun kolom yang diusulkan yaitu:
* Category
* City
"""

df_mf = merged_preferensi[['User_Id','Place_Id','Place_Ratings',
            'Location','Age','Category','City','Image_Feature']+[i for i in preferensi_user.columns if i != 'P_User_Id']]

# Dummy Variable
df_mf = pd.get_dummies(df_mf, prefix=None, prefix_sep='', drop_first=True, columns=['Category','City'])


X=df_mf[[i for i in df_mf.columns if i != 'Place_Ratings']]
y=df_mf['Place_Ratings']
print(X.shape)
print(y.shape)

"""> Setelah itu, dilakukannya pembagian data untuk melakukan pelatihan dan pengujian model dengan ratio 4:1 dengan algoritma randomness sebesar 150"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=150, shuffle=True)
X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.2, random_state=150, shuffle=True)

print(f"Train: {X_train.shape}")
print(f"Y: {X_valid.shape}")
print(f"Testing: {X_test.shape}")

y_train_enc = tf.keras.utils.to_categorical(y_train)
y_valid_enc = tf.keras.utils.to_categorical(y_valid)
y_test_enc = tf.keras.utils.to_categorical(y_test)

print(f"Y: {y_train.shape}")
print(f"Y: {y_valid.shape}")
print(f"Y: {y_test.shape}")

"""> Adapun data user yang digunakan untuk melakukan validasi ketepatan pemberian rekomendasi item kepada pengguna. Dalam kasus ini, akan digunakan data user dengan id 275 untuk menjadi sampel validasi."""

X_test.groupby('User_Id')['Place_Id'].count().sort_values(axis=0, ascending=False)

# Data testing random user
user_id = 146

"""# Model - Configuration

> Dalam melalukan pelatihan, pengujian, dan validasi model, dibutuhkannya matrik pengujian untuk memberikan informasi performa model yang telah dilakukan training. Adapun matrik yang digunakan antara lain;

* RMSE (Root Mean Squared Error)
* Accuracy (Ketepatan penempatan rekomendasi tempat wisata)
* True Positive Accuracy (Ketepatan pemberian rekomendasi tempat wisata yang relevan)
* NDGC (Normalized Discounted Cumulative)

> Selain daripada itu, dibutuhkannya visualisasi performa model menggunakan hasil training dan testing model serta visualisasi arsitektur model menggunakan graphviz.
"""

from sklearn.metrics import accuracy_score, label_ranking_average_precision_score, jaccard_score, ndcg_score, average_precision_score

# Visualisasi performa
def visualizePerformance(history):
    
    # Loss
    training_loss = history.history['loss']
    validation_loss = history.history['val_loss']
    epoch = range(1, len(training_loss) + 1)
    
    # Accuracy
    training_acc = history.history['accuracy']
    validation_acc = history.history['val_accuracy']
    epoch_acc = range(1, len(training_acc) + 1)
    
    fig, axs = plt.subplots(1, 2, figsize=(9, 5), sharey=False)
    axs[0].plot(epoch,training_loss, 'r--')
    axs[0].plot(epoch,validation_loss,'b-')
    axs[0].legend(['Training Loss','Validation Loss'])
    
    axs[1].plot(epoch,training_acc, 'r--')
    axs[1].plot(epoch,validation_acc,'b-')
    axs[1].legend(['Training accuracy','Validation accuracy'])
    
    plt.show()
    
    fig.suptitle('Performa Model')

def MetricsEvaluation(model, feature, X_data, y_data, item, min_k, jump_k, user_id=1):
    data = X_data.copy()
    # Sort Place_Ratings
    data['y_aktual'] = y_data
    data = data[data['User_Id'] == user_id]
    data = data.sort_values(by=['y_aktual'], ascending=False)

    feature_model = [data[i] for i in feature if i != 'Image_Feature']

    if 'Image_Feature' in feature:
      X_img = np.array([i for i in data['Image_Feature'].values])
      feature_model = feature_model + [X_img]

    print(f'RMSE Score: {np.sqrt(model.evaluate(feature_model, data["y_aktual"]))}')

    data['y_predicted'] = model.predict(feature_model).reshape(data.shape[0])
    data_prediction = data.sort_values(by=['y_predicted'], ascending=False)

    for k in range(min_k,data.shape[0],jump_k):
      print(f'Top {k}')

      # Acc Ketepatan Urutan Item
      print(f'Accuracy Score: {accuracy_score(data[item][:k].values, data_prediction[item][:k].values) * 100}%')

      # Kebenaran item direkomendasikan
      true_positive = len(list(set(data[item][:k].values) & set(data_prediction[item][:k].values)))
      print(f'Ground Truth Items Frequencies): {true_positive}, {str(round(true_positive/k*100, 3))}%')

      print(f'NDGC Score: {ndcg_score([list(data[item].values)], [list(data_prediction[item].values)], k=k)*100}%', end="\n\n")

def getRecommendation(userId=1, model=False, fitur=[], data=None):
    targetUser = data[data['User_Id']==userId]
    placeId = np.array(targetUser['Place_Id'])
    x= [targetUser[i] for i in fitur]
    recommendation = model.predict(x=x)
    placeRecommendation = [[placeId[i],np.argmax(recommendation[i]),max(recommendation[i])] for i in range(len(recommendation))]
    return placeRecommendation

def generateUserData(userId,age,data):
    length = data.shape[0]+1
    newDf = pd.DataFrame({
        'User_Id':[userId for i in range(1,length)],
        'Place_Id':[i for i in data.Place_Id],
        'Age':[age for i in range(1,length)],
        'Category':[i for i in data.Category]
    })
    
    newDf['Category'] = le.fit_transform(newDf['Category'])
    return newDf

def visualizeModelArchitect(model, model_name):
  model_img_file = model_name
  return tf.keras.utils.plot_model(model, to_file=model_img_file, 
                            show_shapes=True, 
                            show_layer_activations=True, 
                            show_dtype=True,
                            show_layer_names=True)

"""> Untuk menghindari terjadinya overfitting, digunakannya callbacks early stopping untuk menghentikan training apabila tidak ditemukannya perubahan nilai loss. Hal ini juga menghindari penggunaan resources yang tidak efisien dengan tetap melakukan training walaupun tidak ditemukan perubahan"""

# Stop callback
class stopCallbackModel(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('loss')<0.35):
      self.model.stop_training = True

callbackStop = stopCallbackModel()


callbackEarlyStopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', 
    patience=3,
)

"""# Nearest Neighbours
---
Referensi:
* https://github.com/aniketng21/Movie-Recommendation-System-Using-KNN-Algorithm/blob/master/Movie_Recommendation_System.ipynb
* https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors
* https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html
"""

from scipy.sparse import csr_matrix
# from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import NearestNeighbors

data_pivot = pd.pivot_table(df, values='Place_Ratings',fill_value=0.0, index='Place_Name', columns='User_Id')
data_pivot.head()

csr_matrix(data_pivot.values)

modelRecommender = NearestNeighbors(n_neighbors=5, algorithm='brute', metric='cosine', n_jobs=-1)
modelRecommender.fit(csr_matrix(data_pivot.values))

data_pivot.iloc[1,:]

data_pivot

distances, indices = modelRecommender.kneighbors(data_pivot.iloc[1,:].values.reshape(1, -1), n_neighbors = 10)
print(distances, indices)

for place, distance in zip(data_pivot.index[indices.flatten()], distances.flatten()):
  print(place, distance)

print(len(df.User_Id.unique()))
print(len(df.Place_Id.unique()))

"""> Pengujian data user random untuk melihat performa model"""

def generate_user_aktual_data(data, y_aktual, k):
  user = data.copy()
  user['y_aktual'] = y_aktual.values
  return user

"""# Collaborative Filtering Matrix Factorization

## Explicit Feedback
"""

# GMF
input_item = tf.keras.layers.Input(shape=(1,))
input_user = tf.keras.layers.Input(shape=(1,))

# Image Hybrid Features
input_img = tf.keras.layers.Input(shape=(size, size, 3))

# Konfigurasi
batch_size, epochs = 256, 200
factors = 70
learning_rate = 0.01

# Vocab
item_vocab, user_vocab = list(X.Place_Id.unique()), list(X.User_Id.unique())

# age_vocab = list(X.Age.unique())

# Optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate)
# optimizer = tf.keras.optimizers.Adadelta(learning_rate)
# optimizer = tf.keras.optimizers.Adagrad(learning_rate)

"""### Build"""

def generateFullyConnectedLayers(x):
  x = tf.keras.layers.Flatten()(x.layers[-1].output)
  x = tf.keras.layers.Dense(4096)(x)
  x = tf.keras.layers.LeakyReLU()(x)
  x = tf.keras.layers.BatchNormalization()(x)
  x = tf.keras.layers.Dense(4096)(x)
  x = tf.keras.layers.LeakyReLU()(x)
  x = tf.keras.layers.Dense(1000)(x)
  return x

def image_extractor_model(input):
  # Content-based Image Retrieval
  model_img = tf.keras.applications.inception_v3.InceptionV3(
      include_top=False,
      weights='imagenet',
      input_tensor=input,
  )
  
  for i in model_img.layers:
    i._name = i._name + str('_model')
    i.trainable = False
    
  model_img = generateFullyConnectedLayers(model_img)
  model = tf.keras.Model(inputs=input, outputs=model_img)
  return model

def create_model(input_user, input_item, user_vocab, item_vocab, factors):
  """  
    Matrix Factorization
  """
  # User
  x = tf.keras.layers.Embedding(input_dim=len(user_vocab)+1, output_dim=factors)(input_user)
  x = tf.keras.layers.Reshape((factors,))(x)

  # Item
  if(input_item.dtype == tf.string):
    y = tf.keras.Sequential([
        tf.keras.layers.StringLookup(vocabulary=item_vocab, num_oov_indices=1),
        tf.keras.layers.Embedding(input_dim=len(item_vocab)+1, output_dim=factors)
    ])(input_item)
  else:
    y = tf.keras.layers.Embedding(input_dim=len(item_vocab)+1, output_dim=factors)(input_item)
  y = tf.keras.layers.Reshape((factors,))(y)
  
  """
    Neural Network
  """
  # User
  xn = tf.keras.layers.Embedding(input_dim=len(user_vocab)+1, output_dim=factors)(input_user)
  xn = tf.keras.layers.Reshape((factors,))(xn)

  # Item
  if(input_item.dtype == tf.string):
    yn = tf.keras.Sequential([
        tf.keras.layers.StringLookup(vocabulary=item_vocab, num_oov_indices=1),
        tf.keras.layers.Embedding(input_dim=len(item_vocab)+1, output_dim=factors)
    ])(input_item)
  else:
    yn = tf.keras.layers.Embedding(input_dim=len(item_vocab)+1, output_dim=factors)(input_item)
  yn = tf.keras.layers.Reshape((factors,))(yn)

  # MF
  x = tf.keras.layers.Dot(axes=1,normalize=True)([x,y])

  # NN
  xn = tf.keras.layers.Concatenate()([xn,yn])
  xn = tf.keras.layers.Dense(64,activation="relu",
                            kernel_initializer='he_normal',
                            kernel_regularizer="l2")(xn)
  xn = tf.keras.layers.BatchNormalization()(xn)
  xn = tf.keras.layers.Dropout(0.35)(xn)
  xn = tf.keras.layers.Dense(32,activation="relu",
                            kernel_initializer='he_normal',
                            kernel_regularizer="l2")(xn)


  # MF x NN
  x = tf.keras.layers.Concatenate()([x, xn])
  x = tf.keras.layers.Dense(1)(x)


  # Pembuatan model
  model = tf.keras.models.Model(inputs=[input_user,input_item], outputs=x)
  model.compile(optimizer=optimizer,loss='mse',
                metrics=[tf.keras.metrics.RootMeanSquaredError(name='accuracy')]) 
  return model

feature = ['User_Id', 'Place_Id']

model = create_model(input_user, input_item, user_vocab, item_vocab, factors)
history = model.fit(x=[X_train[i] for i in feature], 
                      y=y_train, 
                      batch_size=batch_size, epochs=epochs, 
                      verbose=1, 
                      validation_data=([X_valid[i] for i in feature], y_valid),
                      validation_batch_size=16,
                      callbacks=[callbackEarlyStopping]
                      )

visualizePerformance(history)

# Validasi metriks
MetricsEvaluation(model, feature, X_test, y_test, 'Place_Id', 5, 5, user_id)

visualizeModelArchitect(model, 'GMF.png')

"""#### Ensamble Method

> Percobaan penggunaan ***ensambled method average voting***
"""

model_list = []
for i in range(4):
  print(f'Model {i+1} training')
  model = create_model(input_user, input_item, user_vocab, item_vocab, factors)
  history = model.fit(x=[X_train[i] for i in feature], 
                      y=y_train, 
                      batch_size=batch_size, epochs=epochs, 
                      verbose=0, 
                      validation_data=([X_valid[i] for i in feature], y_valid),
                      validation_batch_size=16,
                      callbacks=[callbackEarlyStopping])
  model_list.append(model)

"""##### Bagging"""

result = [i.predict(x=[X_test['User_Id'], X_test['Place_Id']]) for i in model_list]

from sklearn.metrics import mean_squared_error

result = [(a[0], b[0], c[0], d[0]) for a, b, c, d in zip(result[0], result[1], result[2], result[3])]

# Mean
average_result = [np.mean(i) for i in result]


# Bagging
print(f'Average Ensambled: {mean_squared_error(y_test, average_result)}')
print(f'Max Ensambled: {mean_squared_error(y_test, average_result)}')
print(f'Min Ensambled: {mean_squared_error(y_test, average_result)}')

"""# Hybrid GMF Product Text Feature

> Evaluasi model Hybrid yang menggunakan fitur produk berupa kategori dan lokasi kota tempat wisata berada
"""

model = create_model(input_user, input_item, user_vocab, item_vocab, factors)
model.summary()

# [5:11] Personalization
X_train.columns

personalized_cols = ['P_Budaya','P_Taman Hiburan', 'P_Cagar Alam', 'P_Bahari', 'P_Pusat Perbelanjaan','P_Tempat Ibadah']

# Product Hybrid Features
input_item_features = tf.keras.layers.Input(shape=(9+len(personalized_cols),))

# Product Category - City
x = tf.keras.layers.Dense(8, activation='relu')(input_item_features)
x = tf.keras.layers.Dropout(0.25)(x)
x = tf.keras.layers.Dense(8, activation='relu')(x)

x = tf.keras.layers.Concatenate()([model.layers[-4].output, model.layers[-3].output, x])
x = tf.keras.layers.Dense(1, activation='linear')(x)

model_combined = tf.keras.Model(inputs=model.inputs+[input_item_features], outputs=x)
model_combined.compile(
    loss='mse',
    optimizer=optimizer,
    metrics=[tf.keras.metrics.RootMeanSquaredError(name='accuracy')]
)

feature = ['User_Id','Place_Id',['CategoryBudaya', 'CategoryCagar Alam',
       'CategoryPusat Perbelanjaan', 'CategoryTaman Hiburan',
       'CategoryTempat Ibadah', 'CityJakarta', 'CitySemarang', 'CitySurabaya',
       'CityYogyakarta']+personalized_cols]

history = model_combined.fit(
    x=[X_train[i] for i in feature],
    y=y_train,
    batch_size=batch_size,
    epochs=epochs,
    verbose=2,
    validation_data=([X_valid[i] for i in feature], y_valid),
    validation_batch_size=16,
    callbacks=[callbackEarlyStopping],
)

# Grafik performa training-testing yang berisi loss (mse) dan accuracy (rmse)
visualizePerformance(history)

# Validasi matrik
MetricsEvaluation(model_combined, feature, X_test, y_test, 'Place_Id', 5, 5, user_id)

# Visualisasi arsitektur model
visualizeModelArchitect(model_combined, 'ProductFeatures.png')

"""# Hybrid Image

> Penggunaan fitur image yang diekstrak dari fine-tuned Inception v3 model untuk ditambahkan dalam embeddings
* Arsitektur ini menggunakan gabungan MF dan Neural Network
"""

X_train_img = np.array([i for i in X_train['Image_Feature'].values])
# X_test_img = np.array([i for i in X_test['Image_Feature'].values])
X_valid_img = np.array([i for i in X_valid['Image_Feature'].values])

X_train_img.shape

"""
  Image Feature Extractor
"""
model_image = image_extractor_model(input_img)

"""
  Matrix Factorization
"""
# User
x = tf.keras.layers.Embedding(input_dim=len(user_vocab)+1, output_dim=factors)(input_user)


# Item
input = tf.keras.layers.Concatenate(axis=-1)([input_item, model_image.layers[-1].output])
y = tf.keras.layers.Embedding(input_dim=len(item_vocab)+1, output_dim=factors, input_length=2)(input)

# MF
x = tf.math.multiply(x, y)

"""
  Neural Network
"""
# User
xn = tf.keras.layers.Embedding(input_dim=len(user_vocab)+1, output_dim=factors)(input_user)

# Item
yn = tf.keras.layers.Embedding(input_dim=len(item_vocab)+1, output_dim=factors, input_length=2)(input)
yn = tf.keras.layers.Flatten()(yn)
yn = tf.expand_dims(yn, axis=1)

# Concate User x Item
xn = tf.concat([xn,yn], axis=-1)

xn = tf.keras.layers.Dense(64,activation="relu",
                          kernel_initializer='he_normal',
                          kernel_regularizer="l2")(xn)
xn = tf.keras.layers.BatchNormalization()(xn)
xn = tf.keras.layers.Dropout(0.35)(xn)
xn = tf.keras.layers.Dense(32,activation="relu",
                          kernel_initializer='he_normal',
                          kernel_regularizer="l2")(xn)


# # MF x NN x Image Feature Extractor
x = tf.keras.layers.Flatten()(x)
x = tf.expand_dims(x, axis=1)
x = tf.keras.layers.Concatenate()([x, xn])
x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(1, activation='linear')(x)


# Pembuatan model
model = tf.keras.models.Model(inputs=[input_user,input_item,input_img], outputs=x)
model.compile(optimizer=optimizer,loss='mse',metrics=[tf.keras.metrics.RootMeanSquaredError(name='accuracy')])

model.summary()

feature = ['User_Id','Place_Id','Image_Feature']

feature_train = [X_train[i] for i in feature if i != 'Image_Feature'] + [X_train_img]
feature_valid = [X_valid[i] for i in feature if i != 'Image_Feature'] + [X_valid_img]

item = 'Place_Id'
k = 5

history = model.fit(x=feature_train,
                    y=y_train, 
                    batch_size=batch_size, epochs=epochs, 
                    verbose=1, 
                    validation_data=(feature_valid, y_valid),
                    validation_batch_size=16,
                    callbacks=[callbackEarlyStopping]
                    )

# Visualisasi performa
visualizePerformance(history)

# Validasi matrik
MetricsEvaluation(model, feature, X_test, y_test, 'Place_Id', 5, 5, user_id)

# Visualisasi arsitektur model
visualizeModelArchitect(model, 'ImageCollaborative.png')

"""## MF Image"""

"""
  Image Feature Extractor
"""
model_image = image_extractor_model(input_img)


"""
  Matrix Factorization
"""
# User
x = tf.keras.layers.Embedding(input_dim=len(user_vocab)+1, output_dim=factors)(input_user)

# Item
input = tf.keras.layers.Concatenate(axis=-1)([input_item, model_image.layers[-1].output])
y = tf.keras.layers.Embedding(input_dim=len(item_vocab)+1, output_dim=factors, input_length=2)(input)

# MF
x = tf.math.multiply(x, y)

x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(1, activation='linear')(x)


# Pembuatan model
model = tf.keras.models.Model(inputs=[input_user,input_item,input_img], outputs=x)
model.compile(optimizer=optimizer,loss='mse',metrics=[tf.keras.metrics.RootMeanSquaredError(name='accuracy')])

model.summary()

history = model.fit(x=feature_train, 
                      y=y_train, 
                      batch_size=batch_size, epochs=epochs, 
                      verbose=1, 
                      validation_data=(feature_valid, y_valid),
                      validation_batch_size=16,
                      callbacks=[callbackEarlyStopping]
                    )

# Visualisasi performa
visualizePerformance(history)

# Validasi metrik
MetricsEvaluation(model, feature, X_test, y_test, 'Place_Id', 5, 5, user_id)

# Visualisasi arsitektur
visualizeModelArchitect(model, 'MFxImage.png')

"""## Hybrid Personalized Image Recommender"""

shape = len(X_train.columns[5:11])
shape

input_personalized = tf.keras.layers.Input(shape=(shape,))

"""
  Image Feature Extractor
"""
model_image = image_extractor_model(input_img)


"""
  Matrix Factorization
"""
# User
x = tf.keras.layers.Embedding(input_dim=len(user_vocab)+1, output_dim=factors)(input_user)

# Item
input = tf.keras.layers.Concatenate(axis=-1)([input_item, model_image.layers[-1].output])
y = tf.keras.layers.Embedding(input_dim=len(item_vocab)+1, output_dim=factors, input_length=2)(input)

# MF
x = tf.math.multiply(x, y)

"""
  Neural Network
"""
# User
xn = tf.keras.layers.Embedding(input_dim=len(user_vocab)+1, output_dim=factors)(input_user)

# Item
yn = tf.keras.layers.Embedding(input_dim=len(item_vocab)+1, output_dim=factors, input_length=2)(input)
yn = tf.keras.layers.Flatten()(yn)
yn = tf.expand_dims(yn, axis=1)


xn = tf.concat([xn,yn], axis=-1) # MF x NN

xn = tf.keras.layers.Dense(64,activation="relu",
                          kernel_initializer='he_normal',
                          kernel_regularizer="l2")(xn)
xn = tf.keras.layers.BatchNormalization()(xn)
xn = tf.keras.layers.Dropout(0.35)(xn)
xn = tf.keras.layers.Dense(32,activation="linear",
                          kernel_initializer='he_normal',
                          kernel_regularizer="l2")(xn)

"""
  Product Features
"""
input_item_features = tf.keras.layers.Input(shape=(9,))
f = tf.keras.layers.Dense(32, activation='relu')(input_item_features)
f = tf.keras.layers.Dropout(0.25)(f)
f = tf.keras.layers.Dense(32, activation='relu')(f)

# """
#   Personalization Features
# """
# p = tf.keras.layers.Dense(32, activation='relu')(input_personalized)
# p = tf.keras.layers.Dropout(0.25)(p)
# p = tf.keras.layers.Dense(32, activation='relu')(p)

# Age
input_age = {} 
input_age['Age'] = tf.keras.layers.Input(shape=(1,))

age_bucketized = tf.feature_column.numeric_column('Age')
feature_columns = [tf.feature_column.bucketized_column(age_bucketized, boundaries=[i for i in range(df['Age'].min(),df['Age'].max()+1, 6)])]
z = tf.keras.layers.DenseFeatures(feature_columns)

# convert FeatureColumns into a single tensor layer
z = z(input_age)

# # MF x NN x Image Feature Extractor
x = tf.keras.layers.Flatten()(x)
x = tf.expand_dims(x, axis=1)

f = tf.expand_dims(f, axis=1)

z = tf.expand_dims(z, axis=1)

x = tf.keras.layers.Concatenate()([x, xn, f, z])
x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(1, activation='linear')(x)


# Pembuatan model
model = tf.keras.models.Model(inputs=[input_user,input_item,input_item_features,input_age['Age'],input_img], outputs=x)
model.compile(optimizer=optimizer,loss='mse',metrics=[tf.keras.metrics.RootMeanSquaredError(name='accuracy')])

model.summary()

feature = ['User_Id', 'Place_Id',['CategoryBudaya', 'CategoryCagar Alam',
       'CategoryPusat Perbelanjaan', 'CategoryTaman Hiburan',
       'CategoryTempat Ibadah', 'CityJakarta', 'CitySemarang', 'CitySurabaya',
       'CityYogyakarta'],'Age','Image_Feature']


feature_train = [X_train[i] for i in feature if i != 'Image_Feature'] + [X_train_img]
feature_valid = [X_valid[i] for i in feature if i != 'Image_Feature'] + [X_valid_img]

history = model.fit(x=feature_train, 
                      y=y_train, 
                      batch_size=batch_size, epochs=epochs, 
                      verbose=1, 
                      validation_data=(feature_valid, y_valid),
                      validation_batch_size=16,
                      callbacks=[callbackEarlyStopping]
                    )

# Visualisasi performa
visualizePerformance(history)

# Validasi metrik
MetricsEvaluation(model, feature, X_test, y_test, 'Place_Id', 5, 5, user_id)

# Visualisasi arsitektur
visualizeModelArchitect(model, 'MFxImage.png')

"""# Content-Based IR"""

input_img = tf.keras.layers.Input(shape=(size, size, 3))

# Image Model
model_image = image_extractor_model(input_img)
model_image.summary()

def image_extract_feature(img, model):
  preprocessed = tf.keras.applications.vgg19.preprocess_input(img.reshape(1, size, size, 3))
  features = model.predict(preprocessed, use_multiprocessing=True)
  return features

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# images_feature = {}
# for index, i in enumerate(tourism_images):
#   feature = image_extract_feature(i, model_image)
#   images_feature[str(index)] = feature

index_images = np.array(list(images_feature.keys()))
feature = np.array(list(images_feature.values()))
print(feature.shape)

# Pengubahan dimensi
feature = feature.reshape(-1, feature.shape[-1])
print(feature.shape)

from sklearn.decomposition import PCA

# Pengurangan dimensi untuk menghindari curse of dimensionality (memilih yang terpenting)
model_pca = PCA(n_components=100, random_state=100)
x = model_pca.fit_transform(feature)

from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances

class CalculateImageSimilarity():
  def __init__(self, image_feature):
    self.feature = image_feature
  
  def similar_result(self, target_image_index, metric='cosine_sim', ascending=False, k=5):
     result = self.calculate_distance(target_image_index, metric)
     result = self.sort_reformat_index(result, ascending)
     return result[:k]


  def calculate_distance(self, image_index, metric):
    result = 0
    if metric == 'cosine_sim':
      result = cosine_similarity(self.feature[image_index:image_index+1], self.feature)
    
    elif metric == 'euclidean':
      result = euclidean_distances(self.feature[image_index:image_index+1], self.feature)
    
    return result[0]
  
  def sort_reformat_index(self, matrix_distance, ascending):
    similarSites = [(index, i) for index, i in enumerate(matrix_distance)]
    similarSites.sort(reverse=ascending, key = lambda x:x[1])
    return similarSites

imageIndex=2
model_matrix_calculation = CalculateImageSimilarity(feature)
model_matrix_calculation.similar_result(imageIndex, metric='cosine_sim', ascending=True, k=5)

model_matrix_calculation.similar_result(imageIndex, metric='euclidean', ascending=False, k=5)

"""# Custom Pairwise WARP Loss"""

def warp_loss(y_true, y_pred):
  pass

import torch
import torch.nn as nn
from torch.autograd import Variable, Function
import random

class WARP(Function): 
    '''
    autograd function of WARP loss
    '''
    @staticmethod
    def forward(ctx, input, target, max_num_trials = None):
        
        batch_size = target.size()[0]
        if max_num_trials is None: 
            max_num_trials = target.size()[1] - 1
        
        positive_indices = torch.zeros(input.size())
        negative_indices = torch.zeros(input.size())
        L = torch.zeros(input.size()[0])
        
        all_labels_idx = np.arange(target.size()[1])
        
        Y = float(target.size()[1])
        J = torch.nonzero(target)

        for i in range(batch_size): 
            
            msk = np.ones(target.size()[1], dtype = bool)
            
            # Find the positive label for this example
            j = J[i, 1]
            positive_indices[i, j] = 1
            msk[j] = False
            
            # initialize the sample_score_margin
            sample_score_margin = -1
            num_trials = 0
            
            neg_labels_idx = all_labels_idx[msk]

            while ((sample_score_margin < 0) and (num_trials < max_num_trials)):
                 
                #randomly sample a negative label
                neg_idx = random.sample(neg_labels_idx, 1)[0]
                msk[neg_idx] = False
                neg_labels_idx = all_labels_idx[msk]
                
                num_trials += 1
                # calculate the score margin 
                sample_score_margin = 1 + input[i, neg_idx] - input[i, j] 
            
            if sample_score_margin < 0:
                # checks if no violating examples have been found 
                continue
            else: 
                loss_weight = np.log(math.floor((Y-1)/(num_trials)))
                L[i] = loss_weight
                negative_indices[i, neg_idx] = 1
                
        loss = L * (1-torch.sum(positive_indices*input, dim = 1) + torch.sum(negative_indices*input, dim = 1))
        
        ctx.save_for_backward(input, target)
        ctx.L = L
        ctx.positive_indices = positive_indices
        ctx.negative_indices = negative_indices
        
        return torch.sum(loss , dim = 0, keepdim = True)

    # This function has only a single output, so it gets only one gradient 
    @staticmethod
    def backward(ctx, grad_output):
        input, target = ctx.saved_variables
        L = Variable(torch.unsqueeze(ctx.L, 1), requires_grad = False)

        positive_indices = Variable(ctx.positive_indices, requires_grad = False) 
        negative_indices = Variable(ctx.negative_indices, requires_grad = False)
        grad_input = grad_output*L*(negative_indices - positive_indices)

        return grad_input, None, None    

      
class WARPLoss(nn.Module): 
    def __init__(self, max_num_trials = None): 
        super(WARPLoss, self).__init__()
        self.max_num_trials = max_num_trials
        
    def forward(self, input, target): 
        return WARP.apply(input, target, self.max_num_trials)

stop